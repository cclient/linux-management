https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-RunningHive


hadoop namenode -format

/usr/local/hadoop-2.7.0/sbin/start-all.sh

  $ $HADOOP_HOME/bin/hadoop fs -mkdir       /tmp
  $ $HADOOP_HOME/bin/hadoop fs -mkdir       /user/hive/warehouse
  $ $HADOOP_HOME/bin/hadoop fs -chmod g+w   /tmp
  $ $HADOOP_HOME/bin/hadoop fs -chmod g+w   /user/hive/warehouse

   $ $HIVE_HOME/bin/hive

   出错，
root@debian03:/usr/local/apache-hive-1.2.0-bin# hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.0/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/spark-1.3.1-bin-hadoop2.6/lib/spark-assembly-1.3.1-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]

因为hadoop 和spark都有SLF4J
注掉/etc/profile中的 spark环境变量reboot 解决




hadoop fs -ls /
出错
15/08/21 02:33:54 WARN conf.Configuration: core-site.xml:an attempt to override final parameter: fs.defaultFS;  Ignoring.
改core-site.xml 中的

  <property>
<name>fs.defaultFS</name>
        <value>hdfs://172.16.231.176</value>
        <final>true</final>
    </property>


